{"cells":[{"cell_type":"markdown","metadata":{"id":"Sy9xLUgwZiMa"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVLIbWft_e2K"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","\n","from sklearn.model_selection import StratifiedKFold\n","from keras.callbacks import EarlyStopping\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"R8Eix89JZnzM"},"source":["### Data Preparation"]},{"cell_type":"code","source":["#(NT,nt)   A - 0\n","#(NT,rp)   B - 1\n","#(NT,rt)   C - 2\n","#(NT,tw)   D - 3\n","\n","#(RP,nt)   E - 4\n","#(RP,rp)   F - 5\n","#(RP,rt)   G - 6\n","#(RP,tw)   H - 7\n","   \n","#(RT,nt)   I - 8\n","#(RT,rp)   J - 9\n","#(RT,rt)   K - 10\n","#(RT,tw)   L - 11\n","\n","def reconstruct_traj(row):\n","    f_i=row\n","    f_i=f_i[1:]\n","    f_i = f_i[:-1]\n","    s=str(f_i)\n","    s=s.replace(\" \", \"\")\n","    x = s.split(\"[\")\n","\n","    t=[]\n","    for i in range(0,len(x)):\n","        r=x[i].replace(\"]\", \"\").split(\",\")\n","        if len(r) >=2:\n","            t.append([r[0],r[1]])\n","    \n","    return t\n","\n","def remove_action(traj):   ##keep only the S_A pair\n","    seq=[]\n","    #print(traj)\n","    for i in range(len(traj)):\n","        seq.append(int(traj[i][0]))\n","    return seq\n","\n","users_traj = pd.read_csv(\"/content/drive/MyDrive/Trajectories+code/users_all_trajectories.csv\") \n","trolls_traj=pd.read_csv(\"/content/drive/MyDrive/Trajectories+code/trolls_all_trajectories.csv\")\n","\n","users_traj['state_sequence'] = users_traj.apply(lambda row : reconstruct_traj(row['state_sequence']), axis = 1)\n","trolls_traj['state_sequence'] = trolls_traj.apply(lambda row : reconstruct_traj(row['state_sequence']), axis = 1)\n","\n","users_traj['sequence_numbers'] = users_traj.apply(lambda row : remove_action(row['state_sequence']), axis = 1)\n","trolls_traj['sequence_numbers'] = trolls_traj.apply(lambda row : remove_action(row['state_sequence']), axis = 1)\n","\n","users_traj['label'] = 0\n","trolls_traj['label']= 1\n","\n","frames = [users_traj, trolls_traj]\n","data = pd.concat(frames)\n","data = data.sample(frac=1).reset_index(drop=True)"],"metadata":{"id":"mX2Qm2r7TCpe","executionInfo":{"status":"ok","timestamp":1645254464642,"user_tz":-120,"elapsed":13929,"user":{"displayName":"Fatima Ezzeddine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioZXUhYz2ErH-eMK_AWdnoExp4g9YUFM11ibzR4g=s64","userId":"05338388484673605564"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qGxXwm2ZL_Z"},"source":["### Trajectory Formation\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"TdXPiCdEe_Pd","executionInfo":{"status":"ok","timestamp":1645254555868,"user_tz":-120,"elapsed":91252,"user":{"displayName":"Fatima Ezzeddine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioZXUhYz2ErH-eMK_AWdnoExp4g9YUFM11ibzR4g=s64","userId":"05338388484673605564"}}},"outputs":[],"source":["#preparing the splitted sequences\n","users = data[data['label']==0].reset_index(drop=True)\n","trolls = data[data['label']==1].reset_index(drop=True)\n","max_length = 200 #trajectory Length: L. LSTM input\n","new_data = pd.DataFrame(columns=['sequence_numbers','label'])\n","count=0\n","#users\n","for i in range(len(users)):\n","  user = users.loc[i]\n","  traj = users.loc[i]['sequence_numbers']\n","  l = len(traj)\n","\n","  if l <= max_length: #if trajectory length less then max_lenght add it as it is\n","    res=traj\n","    new_data.loc[count]=[res,0]\n","    count=count+1\n","\n","  else:  #else split it into records\n","    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n","    for j in range(len(res)):\n","      new_data.loc[count]=[res[j].tolist(),0]\n","      count=count+1\n","      \n","new_data_trolls=pd.DataFrame(columns=['sequence_numbers','label'])\n","\n","count=0\n","#trolls\n","for i in range(len(trolls)):\n","  user=trolls.loc[i]\n","  traj = trolls.loc[i]['sequence_numbers']\n","  l = len(traj)\n","\n","  if l <= max_length:  #if trajectory length less then max_lenght add it as it is\n","    res=traj\n","    new_data_trolls.loc[count]=[res,1]\n","    count=count+1\n","\n","  else: #else split it into records\n","    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n","    for j in range(len(res)):\n","      new_data_trolls.loc[count]=[res[j].tolist(),1]\n","      count=count+1\n","\n","#all_data is the dataframe that contains the extracted sequences\n","frames = [new_data, new_data_trolls]\n","all_data = pd.concat(frames)\n","all_data = all_data.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F05PgNHTxrCZ"},"outputs":[],"source":["print (\" After Splitting --> Users : \" + str(len(new_data)) +\" Trolls : \" +str(len(new_data_trolls)))"]},{"cell_type":"markdown","source":["### Trajectory Classification\n","Training with 75% train and 25% test"],"metadata":{"id":"Ap2AOnT5aEIj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9F6Z-jv6P3I"},"outputs":[],"source":["Xx=all_data['sequence_numbers'].values\n","Y=all_data['label'].values\n","\n","max_length = 200\n","X = pad_sequences(Xx, maxlen=max_length, padding='post')\n","# Split data into 75% training & 25% test\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n","\n","x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n","x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n","\n","x_train=np.asarray(x_train).astype(np.int)\n","y_train=np.asarray(y_train).astype(np.int)\n","\n","x_test=np.asarray(x_test).astype(np.int)\n","y_test=np.asarray(y_test).astype(np.int)\n","\n","\n","# build the neural network\n","model = Sequential()\n","model.add(LSTM(100, return_sequences = True, input_shape = (x_train.shape[1],1 )))\n","model.add(Dropout(rate = 0.2))\n","model.add(LSTM(75,return_sequences = True))\n","model.add(Dropout(rate = 0.2))\n","model.add(LSTM(75,return_sequences = True))\n","model.add(Dropout(rate = 0.2))\n","model.add(LSTM(50))\n","model.add(Dropout(rate = 0.2))\n","model.add(Dense(1, activation = 'sigmoid'))\n","es = EarlyStopping(monitor='auc', mode='max', patience=20, verbose=1) ## patience can be varied\n","model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics=[\"acc\",\"AUC\",\"FalseNegatives\",\"FalsePositives\",\"TrueNegatives\",\"TruePositives\",\"Precision\",\"Recall\"])\n","history = model.fit(x = x_train, y = y_train,callbacks=[es], batch_size = 100, epochs = 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":558,"status":"aborted","timestamp":1645254384898,"user":{"displayName":"Fatima Ezzeddine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioZXUhYz2ErH-eMK_AWdnoExp4g9YUFM11ibzR4g=s64","userId":"05338388484673605564"},"user_tz":-120},"id":"eU2x-fw-wNrd"},"outputs":[],"source":["results = model.evaluate(x_test, y_test, batch_size=100)  #model evaluation on the test set\n","print(results)"]},{"cell_type":"markdown","source":["### Stratified Cross validation"],"metadata":{"id":"cng4dR0zzsrl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWcajVP-Zp1W"},"outputs":[],"source":["Xx=all_data['sequence_numbers'].values\n","y=all_data['label'].values\n","\n","y=np.asarray(y).astype(np.int)\n","max_length = 200\n","X = pad_sequences(Xx, maxlen=max_length, padding='post')\n","\n","#metrics\n","Acc =[]\n","Pre =[]\n","Rec = []\n","Auc = []\n","F1 =[]\n","FI =[]\n","TPR = []\n","TNR = []\n","Loss=[]\n","\n","# Instantiate the cross validator\n","skf = StratifiedKFold(n_splits=10, shuffle=True)\n","\n","# Loop through the indices the split() method returns\n","for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n","\n","    # Generate batches from indices\n","    x_train, x_test = X[train_indices], X[val_indices]\n","    y_train, y_test = y[train_indices], y[val_indices]\n","\n","    x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n","    x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n","\n","    x_train=np.asarray(x_train).astype(np.int)\n","    y_train=np.asarray(y_train).astype(np.int)\n","\n","    x_test=np.asarray(x_test).astype(np.int)\n","    y_test=np.asarray(y_test).astype(np.int)\n","    # build the neural network\n","    model = Sequential()\n","    model.add(LSTM(100, return_sequences = True, input_shape = (x_train.shape[1],1 )))\n","    model.add(Dropout(rate = 0.2))\n","    model.add(LSTM(75,return_sequences = True))\n","    model.add(Dropout(rate = 0.2))\n","    model.add(LSTM(75,return_sequences = True))\n","    model.add(Dropout(rate = 0.2))\n","    model.add(LSTM(50))\n","    model.add(Dropout(rate = 0.2))\n","    model.add(Dense(1, activation = 'sigmoid'))\n","    es = EarlyStopping(monitor='auc', mode='max', patience=20, verbose=1)\n","    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics=[\"acc\",\"AUC\",\"FalseNegatives\",\"FalsePositives\",\"TrueNegatives\",\"TruePositives\",\"Precision\",\"Recall\"])\n","    model.fit(x = x_train, y = y_train,callbacks=[es], batch_size = 100,verbose=1, epochs = 50)\n","    history = model.evaluate(x_test, y_test, batch_size=100)\n","    Loss.append(history[0])\n","\n","    acc= history[1]\n","    Acc.append(acc)\n","\n","    auc = history[2]\n","    Auc.append(auc)\n","\n","    fn= history[3]\n","    fp = history[4]\n","    tn = history[5]\n","    tp = history[6]\n","\n","\n","    tpr = tp/float(tp+fn)\n","    tnr = tn/float(tn+fp)\n","\n","    TPR.append(tpr)\n","    TNR.append(tnr)\n","\n","    prec= history[7]\n","    Pre.append(prec)\n","\n","    rec = history[8]\n","    Rec.append(rec)\n","    try:\n","      f1 = 2 * ((prec*rec)/(prec+rec))\n","    except ZeroDivisionError:\n","      f1 = 0\n","    F1.append(f1)\n","\n","print(\"LSTM With Trajectory classification\")\n","print(\"accuracy\",np.mean(Acc),np.std(Acc))\n","print(\"precision\",np.mean(Pre),np.std(Pre))\n","print(\"recall\",np.mean(Rec),np.std(Rec))\n","print(\"f1\",np.mean(F1),np.std(F1))\n","print(\"AUC\",np.mean(Auc),np.std(Auc))\n","print(\"TPR\",np.mean(TPR),np.std(TPR))\n","print(\"TNR\",np.mean(TNR),np.std(TNR))"]},{"cell_type":"markdown","source":["### Troll Score Computation and CDF plot"],"metadata":{"id":"IHdh08_Y0koi"}},{"cell_type":"code","source":["#### sliding window validation\n","\n","max_length=200\n","\n","output=pd.DataFrame(columns=['screen_name','sequence_numbers','label','troll_score'])\n","\n","counter=0\n","users_acc=[]\n","trolls_acc=[]\n","\n","users = data[data['label']==0].reset_index(drop=True)\n","trolls = data[data['label']==1].reset_index(drop=True)\n","\n","#computing the troll score for the users\n","#sliding window\n","#compute the ratio of how many time the sequences classified as troll\n","\n","for i in range(len(users)):\n","  user=users.loc[i]\n","  traj = users.loc[i]['sequence_numbers']\n","  l = len(traj)\n","\n","  if l <= 200:\n","    res=traj\n","\n","  else:\n","    res=[]\n","    for i in range(l-199):\n","      res.append(traj[i:i+200])\n","\n","    x_test = pad_sequences(res, maxlen=max_length, padding='post')\n","    x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n","    #x_test=np.asarray(res).astype(np.int)\n","    y_test=np.asarray([1]*len(x_test)).astype(np.int)  \n","    history = model.evaluate(x_test, y_test, batch_size=100)\n","    output.loc[counter]=[user['screen_name'],user['sequence_numbers'],0,history[1]]\n","    counter=counter+1\n","    users_acc.append(history[1])\n","\n","#computing the troll score for the trolls\n","#sliding window\n","#compute the ratio of how many time the sequences classified as troll\n","\n","for i in range(len(trolls)):\n","  user=trolls.loc[i]\n","  traj = trolls.loc[i]['sequence_numbers']\n","  l = len(traj)\n","\n","  if l <= 200:\n","    res=traj\n","\n","  else:\n","    #res = np.array_split(traj,l/200)\n","    res=[]\n","    for i in range(l-199):\n","      res.append(traj[i:i+200])\n","    x_test = pad_sequences(res, maxlen=max_length, padding='post')\n","    x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n","    #x_test=np.asarray(res).astype(np.int)\n","    y_test=np.asarray([1]*len(x_test)).astype(np.int)\n","    history = model.evaluate(x_test, y_test, batch_size=100)\n","    output.loc[counter]=[user['screen_name'],user['sequence_numbers'],1,history[1]]\n","    counter=counter+1\n","    trolls_acc.append(history[1])"],"metadata":{"id":"_Lx1OxyB0hxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import matplotlib as mpl\n","\n","mpl.style.use('ggplot')\n","plt.rcParams['figure.figsize'] = (5, 5)\n","\n","# normal distribution\n","# sort the data in ascending order\n","# get the cdf values of y\n","# No of data points used\n","N = len(users_acc)\n","x = np.sort(users_acc)\n","y = np.arange(N) / float(N)\n","plt.plot(x, y,color='blue',label='Users')\n","\n","df=pd.DataFrame(columns=['x','y'])\n","df['x']=x\n","df['y']=y\n","\n","\n","N = len(trolls_acc)\n","x = np.sort(trolls_acc)\n","y = np.arange(N) / float(N)\n","\n","\n","df=pd.DataFrame(columns=['x','y'])\n","df['x']=x\n","df['y']=y\n","\n","plt.plot(x, y,color='red',label='Suspended')\n","\n","font = {'family' : 'normal',\n","        'weight' : 'bold',\n","        'size'   : 12}\n","\n","mpl.rc('font', **font)\n","  \n","# plotting\n","plt.xlabel('Trolls score')\n","plt.ylabel('Density')\n","plt.legend()"],"metadata":{"id":"VlMyzaRs08ST"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Sy9xLUgwZiMa","R8Eix89JZnzM","8qGxXwm2ZL_Z","Ap2AOnT5aEIj","cng4dR0zzsrl","IHdh08_Y0koi"],"name":"Code RNN-LSTM Full Data.ipynb","provenance":[{"file_id":"1d9k31lnTgAD3CeDEOT5Eq56dUP_aj7ln","timestamp":1623600187015}],"authorship_tag":"ABX9TyMUX6GSzqEgibqrx24sQuDR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}